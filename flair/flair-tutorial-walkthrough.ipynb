{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Flair Walkthrough_\n",
    "\n",
    "This notebook will walkthrough how to use [flair](https://github.com/flairNLP/flair), a simple framework built directly on PyTorch, that makes it easy to train your own NLP models and experiment with new approaches using Flair embeddings and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 1: NLP Base Types_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_1_BASICS.md)\n",
    "\n",
    "- two types of objects central to `flair`\n",
    "    - `Sentence` object: holds a textual sentence (essentially a list of `Token`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The grass is green .\" - 5 Tokens\n"
     ]
    }
   ],
   "source": [
    "# the sentence object holds a sentence that we may want to embed or tag\n",
    "from flair.data import Sentence\n",
    "\n",
    "# make a sentence object by passing a whitespace tokenized string\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print out tells us sentence consists of 5 tokens\n",
    "- you can access tokens within sentence via their token id or their index\n",
    "- the print out will include the ID and lexical value of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 4 green\n",
      "Token: 4 green\n"
     ]
    }
   ],
   "source": [
    "# print using the token id\n",
    "print(sentence.get_token(4))\n",
    "# print using the index itself\n",
    "print(sentence[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "Token: 2 grass\n",
      "Token: 3 is\n",
      "Token: 4 green\n",
      "Token: 5 .\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Tokenization_\n",
    "\n",
    "- in cases where text is not already tokenized, can use `use_tokenizer` flag when instantiating `Sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The grass is green .\" - 5 Tokens\n"
     ]
    }
   ],
   "source": [
    "# make sentence object by passing an untokenized string and use_tokenizer flag\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Adding Custom Tokenizers_\n",
    "\n",
    "- can also pass custom tokenizers to `use_tokenizer` flag\n",
    "    - pass a tokenization method instead of a `True` boolean\n",
    "    - check the code of [`flair.data.space_tokenizer`](https://github.com/flairNLP/flair/blob/master/flair/data.py) to get idea of how to implement a wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The grass is green .\" - 5 Tokens\n"
     ]
    }
   ],
   "source": [
    "from flair.data import segtok_tokenizer\n",
    "\n",
    "# make a sentence object by passing in untokenized string and custom tokenizer\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=segtok_tokenizer)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Adding Tags to Tokens_\n",
    "\n",
    "- `Token` has fields for linguistic annotation, such as:\n",
    "    - lemmas\n",
    "    - part-of-speech tags\n",
    "    - named entity tags\n",
    "- can add a tag by specifying tag type & value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grass is green <color> .\n"
     ]
    }
   ],
   "source": [
    "# add NER tag of type color to the word green --> tagged word as an entity of type color\n",
    "sentence[3].add_tag('ner', 'color')\n",
    "\n",
    "# print the sentence with all tags of this type\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Token: 4 green\" is tagged as \"color\" with confidence score \"1.0\"\n"
     ]
    }
   ],
   "source": [
    "# each tag is of class Label, which next to the value has a score indicating confidence\n",
    "# get token 3 in sentence\n",
    "token = sentence[3]\n",
    "\n",
    "# get the ner tag of the token\n",
    "tag = token.get_tag('ner')\n",
    "\n",
    "# print token\n",
    "print(f'\"{token}\" is tagged as \"{tag.value}\" with confidence score \"{tag.score}\"')\n",
    "# our color tag will have a score of 1.0 because we manually added it\n",
    "# if tag is predicted by sequence labeler, the score value will\n",
    "# indicate classifier confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Adding Labels to Sentences_\n",
    "\n",
    "- `Sentence` can have one or more labels that can be used in text classification tasks (for example)\n",
    "- example below will add label `sports` to the sentence\n",
    "    - labeling it as belonging to the `sports` category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports (1.0)\n",
      "world cup (1.0)\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('France is the current World Cup winner.')\n",
    "\n",
    "# add a label to a sentence\n",
    "sentence.add_label('sports')\n",
    "\n",
    "# a sentence can also belong to multiple classes\n",
    "sentence.add_labels(['sports', 'world cup'])\n",
    "\n",
    "# you can also set the labels while initializing the sentence\n",
    "sentence = Sentence('France is the current World Cup winner.', labels=['sports', 'world cup'])\n",
    "\n",
    "# you can print a sentence's labels like this\n",
    "for label in sentence.labels:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 2: Tagging your Text_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md)\n",
    "\n",
    "- below will show how to use `flair`'s pretrained models to tag your text\n",
    "\n",
    "### _Tagging with Pre-trained Sequence Tagging Models_\n",
    "\n",
    "- we'll use pre-trained model for named entity recognition (NER)\n",
    "    - model was trained over the English [CoNLL-03](https://dl.acm.org/doi/10.3115/1119176.1119195) task \n",
    "        - can recognize 4 different entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 18:54:11,441 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n",
      "George <B-PER> Washington <E-PER> went to Washington <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "sentence = Sentence('George Washington went to Washington.', use_tokenizer=True)\n",
    "\n",
    "# use predict() method of the tagger on the sentence\n",
    "# this will add predicted tags to the tokens in the sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with predicted tags\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER-span [1,2]: \"George Washington\"\n",
      "LOC-span [5]: \"Washington\"\n"
     ]
    }
   ],
   "source": [
    "# we can directly get such spans in a tagged sentence like this\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- above indicats that:\n",
    "    - \"George Washington\" is a person (PER)\n",
    "    - \"Washington\" is a location (LOC)\n",
    "- we can get additional information by calling the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'George Washington went to Washington.', 'labels': [], 'entities': [{'text': 'George Washington', 'start_pos': 0, 'end_pos': 17, 'type': 'PER', 'confidence': 0.9967881441116333}, {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'type': 'LOC', 'confidence': 0.9993711113929749}]}\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_dict(tag_type='ner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _List of Pre-Trained Sequence Tagger Models_\n",
    "\n",
    "- you can choose which pre-trained model you load\n",
    "    - can be done by passing appropriate string to the `load()` method of `SequenceTagger` class\n",
    "- for more information on ID's (i.e. the strings to use to load models), check out the list [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#list-of-pre-trained-sequence-tagger-models)\n",
    "\n",
    "### _Tagging a German sentence_\n",
    "\n",
    "- there are pre-trained models for languages other than English\n",
    "- current languages that are supported:\n",
    "    - German, French and Dutch\n",
    "- the cell below is commented out because the model would've taken ~30min to load..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load German model\n",
    "#tagger = SequenceTagger.load('de-ner')\n",
    "\n",
    "# make a German sentence\n",
    "#sentence = Sentence('George Washington ging nach Washington.', use_tokenizer=True)\n",
    "\n",
    "# predict NER tags\n",
    "#tagger.predict(sentence)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "#print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Experimental: Semantic Frame Detection_\n",
    "\n",
    "- has pre-trained model that detect semantic frames in text\n",
    "    - trained using [Propbank 3.0 frames](https://propbank.github.io/)\n",
    "- provides word sense disambiguation for frame evoking words\n",
    "- commented out due to size of model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#tagger = SequenceTagger.load('frame')\n",
    "\n",
    "# make English sentence\n",
    "#sentence1 = Sentence('George returned to Berlin to return his hat.', use_tokenizer=True)\n",
    "#sentence2 = Sentence('He had a look at different hats.', use_tokenizer=True)\n",
    "\n",
    "# predict NER tags\n",
    "#tagger.predict(sentence1)\n",
    "#tagger.predict(sentence2)\n",
    "\n",
    "# print sentence with predicted tags\n",
    "#print(sentence1.to_tagged_string())\n",
    "#print(sentence2.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is what the output for the above cell would look like\n",
    "\n",
    "```\n",
    "George returned <return.01> to Berlin to return <return.02> his hat .\n",
    "\n",
    "He had <have.LV> a look <look.01> at different hats .\n",
    "```\n",
    "\n",
    "- frame detector makes distinction in sentence 1 between different meanings of the word `return`\n",
    "    - `return.01` means returning to a location\n",
    "    - `return.02` means giving something back\n",
    "- in sentence two, frame detector finds light verb construction\n",
    "    - `have` is the light verb\n",
    "    - `look` is a frame evoking word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Tagging a List of Sentences_\n",
    "\n",
    "- may want to tag an entire text corpus\n",
    "    - need to split the corpus into sentences \n",
    "    - then pass a list of `Sentence` objects to `.predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 18:54:15,979 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n",
      "This is a sentence .\n",
      "This is another sentence .\n",
      "I love Berlin <S-LOC> .\n"
     ]
    }
   ],
   "source": [
    "# text of many sentences\n",
    "text = 'This is a sentence. This is another sentence. I love Berlin.'\n",
    "\n",
    "# use library to split into sentences\n",
    "from segtok.segmenter import split_single\n",
    "\n",
    "sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(text)]\n",
    "\n",
    "# predict tags for list of sentences\n",
    "tagger = SequenceTagger.load('ner')\n",
    "tagger.predict(sentences)\n",
    "\n",
    "# iterate through the sentences and print predicted labels\n",
    "for sent in sentences:\n",
    "    print(sent.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `mini_batch_size` parameter of `predict()` method\n",
    "    - can set size of mini-batches passed to the tagger\n",
    "- may have to play around with this paramater to optimize speed\n",
    "\n",
    "### _Tagging with Pre-Trained Text Classification Models_\n",
    "\n",
    "- can use pre-trained model for detecting positive or negative comments\n",
    "    - model was trained over IMDB dataset\n",
    "    - can recognize positive and negative sentiment in English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from flair.models import TextClassifier\n",
    "\n",
    "#classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "#sentence = Sentence('This film hurts. It is so bad that I am confused.', use_tokenizer=True)\n",
    "\n",
    "# predict NEW tags\n",
    "#classifier.predict(setence)\n",
    "\n",
    "# print sentence with predicted labels\n",
    "#print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- above cell should print the following\n",
    "```\n",
    "[NEGATIVE (0.9598667025566101)]\n",
    "```\n",
    "- contains the sentiment and the confidence\n",
    "- here is a [link](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#list-of-pre-trained-text-classification-models) to the list of pre-trained text classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 3: Word Embeddings_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n",
    "\n",
    "### _Embeddings_\n",
    "\n",
    "- all word embedding classes inherit from the `TokenEmbeddings` class\n",
    "    - and implement the `embed()` method, which needs to be called to embed your text\n",
    "    \n",
    "### _Classic Word Embeddings_\n",
    "\n",
    "- are static and word-lvel\n",
    "    - each distinct word gets exactly one pre-computed embedding\n",
    "    - most embedding's fall under this class (including GloVe or Moninos embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706], device='cuda:0')\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374], device='cuda:0')\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635], device='cuda:0')\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
      "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
      "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
      "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
      "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
      "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
      "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
      "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
      "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
      "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
      "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
      "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
      "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
      "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
      "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
      "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
      "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
      "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
      "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
      "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02],\n",
      "       device='cuda:0')\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
      "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
      "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
      "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
      "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
      "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
      "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
      "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
      "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
      "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
      "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
      "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
      "        -0.3924, -0.2339,  0.4730, -0.0288], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# instantiate WordEmbeddings class, pass string ID of embedding you wish to load\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "# init embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# create sentence\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "\n",
    "# embed a sentence using glove\n",
    "glove_embedding.embed(sentence)\n",
    "\n",
    "# now check out embedded tokens\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- above are the print outs of the tokens in the sentence and their embeddings\n",
    "    - GloVe embeddings are `PyTorch` vectors of dimensionality 100\n",
    "- typically, you'll use **two-letter language code** to init an embedding\n",
    "    - `en` for English\n",
    "    - `de` for German\n",
    "- by default, this will initialize FastText embeddings trained over Wikipedia\n",
    "    - can also use FastText embeddings over web crawls with `'-crawl'`\n",
    "    ```\n",
    "    # example\n",
    "    german_embedding = WordEmbeddings('de-crawl')\n",
    "    ```\n",
    "- generally recommend FastText embeddings, or GloVe if you want a smaller model\n",
    "\n",
    "### _Flair Embeddings_\n",
    "\n",
    "- contextual string embeddings --> capture latent syntactic-semantic information that goes beyond standard word embeddings\n",
    "    - are trained without any explicit notion of words, fundamentally model words as sequences of characters\n",
    "    - are contextualized by their surrounding text\n",
    "        - means that _the same word will have different embeddings depending on its contextual use_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"The grass is green .\" - 5 Tokens]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# init embedding\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "\n",
    "# embed words in sentence\n",
    "flair_embedding_forward.embed(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for all supported languages, there is a forward and backward model\n",
    "- if you want to load model for a language using two-letter language code, followed by hyphen and either **forward or backward**\n",
    "```\n",
    "# example\n",
    "flair_embedding_forward = FlairEmbeddings('de-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('de-backward')\n",
    "```\n",
    "\n",
    "### _Stacked Embeddings_\n",
    "\n",
    "- one of the most important concepts of the library\n",
    "- can use to combine different embeddings together\n",
    "    - for example, if you want to use both traditional & contextual string embeddings together\n",
    "- allows you to mix and match, finding a combination of embeddings that gives best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281,  ..., -0.0065, -0.0053,  0.0090],\n",
      "       device='cuda:0')\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405,  ...,  0.0354, -0.0255, -0.0143],\n",
      "       device='cuda:0')\n",
      "Token: 3 is\n",
      "tensor([-5.4264e-01,  4.1476e-01,  1.0322e+00,  ..., -5.3691e-04,\n",
      "        -9.6750e-03, -2.7541e-02], device='cuda:0')\n",
      "Token: 4 green\n",
      "tensor([-0.6791,  0.3491, -0.2398,  ..., -0.0007, -0.1333,  0.0161],\n",
      "       device='cuda:0')\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635,  ...,  0.0005, -0.0177,  0.0032],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, CharacterEmbeddings, StackedEmbeddings\n",
    "\n",
    "# init standard GloVe embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# init Flair forward and backwards embeddings\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# init StackedEmbeddings class and pass it list containing two above embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    glove_embedding,\n",
    "    flair_embedding_forward,\n",
    "    flair_embedding_backward\n",
    "])\n",
    "\n",
    "# create a sentence\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would any single embedding\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 4: List of All Word Embeddings_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md)\n",
    "\n",
    "- primarily a list of all embeddings that are supported in Flair\n",
    "    - [click here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md#overview) for list of these embeddings\n",
    "    \n",
    "### _Combining BERT and Flair_\n",
    "\n",
    "- you can easily mix and match Flair, ELMo, BERT and classic word embeddings\n",
    "    - just instantiate each embedding you wish to combine and use in `StackedEmbedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([0.6800, 0.2429, 0.0012,  ..., 0.3829, 0.4721, 0.2985], device='cuda:0')\n",
      "Token: 2 grass\n",
      "tensor([ 2.9200e-01,  2.2066e-02,  4.5290e-05,  ...,  8.5283e-01,\n",
      "        -5.0724e-02,  3.4476e-01], device='cuda:0')\n",
      "Token: 3 is\n",
      "tensor([-0.5447,  0.0229,  0.0078,  ..., -0.1828,  0.7153,  0.0051],\n",
      "       device='cuda:0')\n",
      "Token: 4 green\n",
      "tensor([1.4772e-01, 1.0973e-01, 8.5618e-04,  ..., 1.0157e+00, 7.5358e-01,\n",
      "        1.1230e-01], device='cuda:0')\n",
      "Token: 5 .\n",
      "tensor([-1.5555e-01,  6.7598e-03,  5.3829e-06,  ..., -6.0930e-01,\n",
      "         9.0591e-01,  1.7857e-01], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings, StackedEmbeddings\n",
    "\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding]\n",
    ")\n",
    "\n",
    "sentence = Sentence('The grass is green.', use_tokenizer=True)\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 5: Document Embeddings_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md)\n",
    "\n",
    "- all document embedding classes inherit from `DocumentEmbeddings` class\n",
    "    - implement `embed()` method which you need to call to embed your text\n",
    "- all embeddings produced with Flair's methods are `PyTorch` vectors\n",
    "    - can be immediately used for training and fine-tuning\n",
    "    \n",
    "### _Document Embeddings_\n",
    "\n",
    "- are created from the embeddings of all words in the document\n",
    "- two different methods to obtain a document embedding from a list of word embeddings\n",
    "    - pooling\n",
    "    - RNN\n",
    "    \n",
    "### _Pooling_\n",
    "\n",
    "- calculates pooling operation over all word embeddings in a document\n",
    "    - default operation is `mean`, gives us the mean of all words in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3197,  0.2621,  0.4037,  ..., -0.0021, -0.0207, -0.0016],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "# create a document embedding using GloVe with Flair\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "\n",
    "# init word embeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "# init document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([\n",
    "    glove_embedding,\n",
    "    flair_embedding_forward,\n",
    "    flair_embedding_backward\n",
    "])\n",
    "\n",
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green. And the sky is blue.', use_tokenizer=True)\n",
    "\n",
    "# embed the sentence with document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the above printed out the embedding of the document\n",
    "- since document embedding is derived from word embeddings...\n",
    "    - dimensionality depends on dimensionality of word embeddings you are using\n",
    "- can also use `min` or `max` pooling (see below)\n",
    "```\n",
    "# example\n",
    "document_embeddings = DocumentPoolEmbeddings([\n",
    "    glove_embedding,\n",
    "    flair_embedding_forward,\n",
    "    flair_embedding_backward],\n",
    "    pooling='min'\n",
    "    )\n",
    "```\n",
    "- can also choose which fine-tuning operation you want\n",
    "    - i.e. which transformation to apply before word embeddings get pooled\n",
    "    - default operation is `linear` transformation\n",
    "    - but if you want simple word embeddings that are not task-trained, you can use a 'nonlinear' transformation instead\n",
    "\n",
    "```\n",
    "# instantiate pre-trained word embeddings\n",
    "embeddings = WordEmbeddings('glove')\n",
    "\n",
    "# document pool embeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='nonlinear')\n",
    "```\n",
    "\n",
    "- if you want word embeddings that are task-trained, you are better off doing no transformation at all\n",
    "\n",
    "```\n",
    "# instantiate one-hot encoded word embeddings\n",
    "embeddings = OneHotEmbeddings(corpus)\n",
    "\n",
    "# document pool embeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='none')\n",
    "```\n",
    "\n",
    "### _RNN_\n",
    "\n",
    "- also support an RNN to obtain a `DocumentEmbeddings`\n",
    "- takes the word embeddings of every token in document as input\n",
    "    - provides its last output state as document embedding\n",
    "- can choose which type of RNN you wish to use\n",
    "- by default, a GRU-type RNN in instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3197,  0.2621,  0.4037,  ..., -0.0021, -0.0207, -0.0016],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "# init GloVe embedding\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# init RNN document embedding\n",
    "document_embedding = DocumentRNNEmbeddings([glove_embedding])\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('The grass is green. And the sky is blue.', use_tokenizer=True)\n",
    "\n",
    "# embed the sentence with document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the above outputs a single embedding for the complete sentence\n",
    "    - embedding dimensionality depends on number of hidden states you are using & whether RNN is bidirectional or not\n",
    "- if you want to use different type of RNN, can set the `rnn_type` parameter in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init GloVe embedding \n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "# create LSTM document embedding\n",
    "document_lstm_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: `DocumentPoolEmbeddings` are immediately meaningful\n",
    "    - `DocumentRNNEmbeddings` need to be tuned on the downstream task\n",
    "    - this happens automatically in Flair if you train a new model with these embeddings\n",
    "    - for an example, click [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model)\n",
    "    - once model is trained, you can access tuned `DocumentRNNEmbeddings` object directly from classifier object and embed sentences\n",
    "\n",
    "```\n",
    "document_embeddings = classifier.document_embeddings\n",
    "\n",
    "sentence = Sentence('The grass is green. And the sky is blue.', use_tokenizer=True)\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())\n",
    "```\n",
    "\n",
    "- `DocumentRNNEmbeddings` have hyper-parameters that can be tuned to improve learning\n",
    "    - `hidden_size`: number of hidden states to run\n",
    "    - `rnn_layers`: number of layers for the RNN\n",
    "    - `reproject_words`: boolean value, indicates whether to reproject the token embeddings in a separate linear layer before putting them into the RNN or not\n",
    "    - `reproject_words_dimension`: output dimension of reprojecting token embeddings; if `None` the same output dimension as before will be taken\n",
    "    - `bidirectional`: boolean value, indicating whether to use bidirectional RNN or not\n",
    "    - `dropout`: dropout value to be used\n",
    "    - `word_dropout`: word dropout value to be used, if `0.0` word dropout is not used\n",
    "    - `locked_dropout`: locked dropout value to be used, if `0.0` locked dropout is not used\n",
    "    - `rnn_type`: one of `RNN` or `LSTM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [_Tutorial 6: Loading Training Data_](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md)\n",
    "\n",
    "- `Corpus` represent dataset you use to train a model\n",
    "    - consists of the following, which correspond to training, validation and testing split during model training:\n",
    "        - `train` sentences\n",
    "        - list of `dev` sentences\n",
    "        - list of `test` sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 21:23:53,988 Reading data from /root/.flair/datasets/ud_english\n",
      "2020-05-20 21:23:53,989 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-05-20 21:23:53,990 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2020-05-20 21:23:53,991 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n"
     ]
    }
   ],
   "source": [
    "# instantiate Universal Dependency Treebank for English as corpus object\n",
    "import flair.datasets\n",
    "corpus = flair.datasets.UD_ENGLISH()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note: first time you call this snippet, it triggers download of the dataset onto hard drive\n",
    "    - then reads train, test and dev splits into `Corpus` corpus which it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543\n",
      "2077\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "# print the number of Sentences in the train split\n",
    "print(len(corpus.train))\n",
    "\n",
    "# print the number of Sentences in the test split\n",
    "print(len(corpus.test))\n",
    "\n",
    "# print the number of Sentences in the dev split\n",
    "print(len(corpus.dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"What if Google Morphed Into GoogleOS ?\" - 7 Tokens\n"
     ]
    }
   ],
   "source": [
    "# can also access Sentence object directly --> print the first Sentence in the testing split\n",
    "print(corpus.test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Sentence` above is fully tagged with syntactic and morphological information\n",
    "    - for example the POS tags\n",
    "- this means that the corpus is tagged and ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What <WP> if <IN> Google <NNP> Morphed <VBD> Into <IN> GoogleOS <NNP> ? <.>\n"
     ]
    }
   ],
   "source": [
    "# print the first Sentence in the testing split\n",
    "print(corpus.test[0].to_tagged_string('pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Helper Functions_\n",
    "\n",
    "- `Corpus` contains useful helper functions\n",
    "    - example: you can downsample the data by calling `downsample()` & passing a ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 21:35:39,420 Reading data from /root/.flair/datasets/ud_english\n",
      "2020-05-20 21:35:39,421 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-05-20 21:35:39,422 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2020-05-20 21:35:39,422 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "--- 1 Original ---\n",
      "Corpus: 12543 train + 2002 dev + 2077 test sentences\n",
      "--- 2 Downsampled ---\n",
      "Corpus: 1254 train + 200 dev + 208 test sentences\n"
     ]
    }
   ],
   "source": [
    "# downsample to 10% of the data\n",
    "downsampled_corpus = flair.datasets.UD_ENGLISH().downsample(0.1)\n",
    "\n",
    "print('--- 1 Original ---')\n",
    "print(corpus)\n",
    "\n",
    "print('--- 2 Downsampled ---')\n",
    "print(downsampled_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for many learning tasks you need to create a target dictionary\n",
    "    - `Corpus` enables you to create your tag or label dictionary, depending on task you want to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:11:09,138 Reading data from /root/.flair/datasets/ud_english\n",
      "2020-05-20 22:11:09,139 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-05-20 22:11:09,140 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2020-05-20 22:11:09,140 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "Dictionary with 21 tags: <unk>, O, PROPN, PUNCT, ADJ, NOUN, VERB, DET, ADP, AUX, PRON, PART, SCONJ, NUM, ADV, CCONJ, X, INTJ, SYM, <START>, <STOP>\n",
      "2020-05-20 22:11:13,814 Reading data from /root/.flair/datasets/conll_03_dutch\n",
      "2020-05-20 22:11:13,814 Train: /root/.flair/datasets/conll_03_dutch/ned.train\n",
      "2020-05-20 22:11:13,815 Dev: /root/.flair/datasets/conll_03_dutch/ned.testa\n",
      "2020-05-20 22:11:13,815 Test: /root/.flair/datasets/conll_03_dutch/ned.testb\n",
      "Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-PER, S-LOC, B-MISC, E-MISC, B-ORG, E-ORG, I-ORG, I-PER, B-LOC, I-LOC, E-LOC, I-MISC, <START>, <STOP>\n",
      "2020-05-20 22:11:24,271 Reading data from /root/.flair/datasets/trec_6\n",
      "2020-05-20 22:11:24,272 Train: /root/.flair/datasets/trec_6/train.txt\n",
      "2020-05-20 22:11:24,273 Dev: None\n",
      "2020-05-20 22:11:24,274 Test: /root/.flair/datasets/trec_6/test.txt\n",
      "2020-05-20 22:11:24,572 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4907/4907 [00:00<00:00, 203930.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:11:24,599 [b'ENTY', b'DESC', b'HUM', b'LOC', b'NUM', b'ABBR']\n",
      "Dictionary with 6 tags: ENTY, DESC, HUM, LOC, NUM, ABBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create tag dictionary for a PoS task\n",
    "corpus = flair.datasets.UD_ENGLISH()\n",
    "print(corpus.make_tag_dictionary('upos'))\n",
    "\n",
    "# create tag dictionary for an NER task\n",
    "corpus = flair.datasets.CONLL_03_DUTCH()\n",
    "print(corpus.make_tag_dictionary('ner'))\n",
    "\n",
    "# create label dictionary for a text classification task\n",
    "corpus = flair.datasets.TREC_6()\n",
    "print(corpus.make_label_dictionary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another useful function is `obtain_statistics()`\n",
    "    - returns a Python dictionary with useful stats about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:12:43,472 Reading data from /root/.flair/datasets/trec_6\n",
      "2020-05-20 22:12:43,473 Train: /root/.flair/datasets/trec_6/train.txt\n",
      "2020-05-20 22:12:43,474 Dev: None\n",
      "2020-05-20 22:12:43,474 Test: /root/.flair/datasets/trec_6/test.txt\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 4907,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"NUM\": 808,\n",
      "            \"ENTY\": 1138,\n",
      "            \"DESC\": 1044,\n",
      "            \"LOC\": 745,\n",
      "            \"HUM\": 1098,\n",
      "            \"ABBR\": 74\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 50043,\n",
      "            \"min\": 3,\n",
      "            \"max\": 37,\n",
      "            \"avg\": 10.198288159771755\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 500,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"NUM\": 113,\n",
      "            \"LOC\": 81,\n",
      "            \"HUM\": 65,\n",
      "            \"DESC\": 138,\n",
      "            \"ENTY\": 94,\n",
      "            \"ABBR\": 9\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 3758,\n",
      "            \"min\": 4,\n",
      "            \"max\": 17,\n",
      "            \"avg\": 7.516\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 545,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"HUM\": 125,\n",
      "            \"ENTY\": 112,\n",
      "            \"DESC\": 118,\n",
      "            \"LOC\": 90,\n",
      "            \"NUM\": 88,\n",
      "            \"ABBR\": 12\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 5592,\n",
      "            \"min\": 4,\n",
      "            \"max\": 31,\n",
      "            \"avg\": 10.260550458715596\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# gather stats on IMDB dataset\n",
    "import flair.datasets\n",
    "corpus = flair.datasets.TREC_6()\n",
    "stats = corpus.obtain_statistics()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _The MultiCorpus Object_\n",
    "\n",
    "- if you want to train multiple tasks at once, you can use `MultiCorpus`\n",
    "- first need to create any number of `Corpus` objects\n",
    "    - after, pass list of `Corpus` to `MultiCorpus` object\n",
    "- the following loads a combination corpus consisting of English, German and Dutch Universal Dependency Treebanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:37,413 Reading data from /root/.flair/datasets/ud_english\n",
      "2020-05-20 22:16:37,414 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2020-05-20 22:16:37,414 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "2020-05-20 22:16:37,415 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2020-05-20 22:16:50,539 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-dev.conllu not found in cache, downloading to /tmp/tmph85vzjc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "882822B [00:00, 58450258.03B/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:50,579 copying /tmp/tmph85vzjc1 to cache at /root/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
      "2020-05-20 22:16:50,581 removing temp file /tmp/tmph85vzjc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:50,869 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-test.conllu not found in cache, downloading to /tmp/tmp2bbi1202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1177197B [00:00, 60654538.92B/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:50,916 copying /tmp/tmp2bbi1202 to cache at /root/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
      "2020-05-20 22:16:50,918 removing temp file /tmp/tmp2bbi1202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:52,011 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-train.conllu not found in cache, downloading to /tmp/tmp83d_t2ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18886219B [00:00, 91103558.25B/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:16:52,254 copying /tmp/tmp83d_t2ad to cache at /root/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
      "2020-05-20 22:16:52,270 removing temp file /tmp/tmp83d_t2ad\n",
      "2020-05-20 22:16:52,273 Reading data from /root/.flair/datasets/ud_german\n",
      "2020-05-20 22:16:52,274 Train: /root/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
      "2020-05-20 22:16:52,274 Test: /root/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
      "2020-05-20 22:16:52,275 Dev: /root/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:02,054 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-dev.conllu not found in cache, downloading to /tmp/tmpdht1ow0b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "971006B [00:00, 60659731.16B/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:02,095 copying /tmp/tmpdht1ow0b to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-dev.conllu\n",
      "2020-05-20 22:17:02,098 removing temp file /tmp/tmpdht1ow0b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:02,685 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-test.conllu not found in cache, downloading to /tmp/tmpj89zwq5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "928283B [00:00, 68023010.94B/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:02,723 copying /tmp/tmpj89zwq5n to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-test.conllu\n",
      "2020-05-20 22:17:02,726 removing temp file /tmp/tmpj89zwq5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:03,719 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-train.conllu not found in cache, downloading to /tmp/tmpczgj4l3j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14959268B [00:00, 103215747.63B/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 22:17:03,893 copying /tmp/tmpczgj4l3j to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-train.conllu\n",
      "2020-05-20 22:17:03,905 removing temp file /tmp/tmpczgj4l3j\n",
      "2020-05-20 22:17:03,908 Reading data from /root/.flair/datasets/ud_dutch\n",
      "2020-05-20 22:17:03,909 Train: /root/.flair/datasets/ud_dutch/nl_alpino-ud-train.conllu\n",
      "2020-05-20 22:17:03,909 Test: /root/.flair/datasets/ud_dutch/nl_alpino-ud-test.conllu\n",
      "2020-05-20 22:17:03,910 Dev: /root/.flair/datasets/ud_dutch/nl_alpino-ud-dev.conllu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "english_corpus = flair.datasets.UD_ENGLISH()\n",
    "german_corpus = flair.datasets.UD_GERMAN()\n",
    "dutch_corpus = flair.datasets.UD_DUTCH()\n",
    "\n",
    "# make multi-corpus consisting of three UDs\n",
    "from flair.data import MultiCorpus\n",
    "multi_corpus = MultiCorpus([\n",
    "    english_corpus, german_corpus, dutch_corpus\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Prepared Datasets_\n",
    "\n",
    "- Flair supports growing list of prepared datasets out of the box\n",
    "    - automatically downloads and sets up the data the first time you call the corresponding constructor ID\n",
    "- Click [here](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md#prepared-datasets) for the list\n",
    "\n",
    "**Note** -- There are a few resources in Tutorial 6 that I couldn't complete due to the tutorials using generic data.\n",
    "\n",
    "**There is a section called [`Reading a Text Classification Dataset`](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md#reading-a-text-classification-dataset) that looks like it may be useful for trying to replicate the spam classifier project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
