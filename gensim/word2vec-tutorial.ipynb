{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Word2Vec Model_\n",
    "\n",
    "This notebook introduces the [`gensim`](https://radimrehurek.com/gensim/index.html) library, which can be used for various NLP tasks, with a specific focus on topic modeling. \n",
    "\n",
    "More specifically, this notebook will follow along with `gensim`'s [Word2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) tutorial, which gives the user hands-on experience not only with the library but with [`Word2Vec`](https://en.wikipedia.org/wiki/Word2vec) as well, which is an algorithm used to learn a word embedding from a text corpus.\n",
    "\n",
    "What `Word2vec` does is take large amounts of unannotated text and attempts to learn the semantic relationships between the words. The outputs are vectors, with one vector per word. For example, it allows us to detect the following relationships:\n",
    "- vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n",
    "- vec(\"Montral Canadiens\") - vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\")\n",
    "\n",
    "Now how does it do this? How is it capable of detecting these relationships? It uses a neural network to embed words in a lower-dimensional vector space. The result is a set of word-vectors that cluster together according to their meanings. In other words, words that are grouped together have similar meanings, while words that are further away from each other are more dissimilar. \n",
    "\n",
    "There are two versions of `Word2vec`, and the `gensim` class implements them both:\n",
    "1. Skip-grams (SG)\n",
    "2. Continuous-bag-of-words (CBOW)\n",
    "\n",
    "Skip-grams takes in a pair of words generated by a moving window across the text data. It trains a 1-hidden-layer neural network based on this, resulting in a predicted probability distribution of words that are nearby to the input (i.e. the first word). Then a virtual one-hot encoding of words goes through a 'project layer' to the hidden layer, and these projection weights are later interpreted as word embeddings. \n",
    "\n",
    "Continuous-bag-of-words is similar in that it also has a 1-hidden-layer neural network. Instead of using a single word as in skip-gram, the training task now uses the average of multiple input context words to predict the center word. Then the projection weights are again turned from one-hot words into averageable vectors, of the same width as the hidden layer, and interpreted as the word embeddings.\n",
    "\n",
    "### _Demo_\n",
    "\n",
    "We are going to download a pre-trained model and play around with it. We'll use `gensim` to fetch the `Word2Vec` model trained on part of the Google News dataset, which covers approximately 3 million words and phrases. Training such a model can take hours, but this one is already available and all we have to do is download it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in word2vec trained on Google News\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "in\n",
      "for\n",
      "that\n",
      "is\n",
      "on\n",
      "##\n",
      "The\n",
      "with\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "# let's retrieve some of the vocabulary of the model\n",
    "for i, word in enumerate(wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can easily obtain vectors for terms the model is familiar with\n",
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word cameroon does not appear in this mode.\n"
     ]
    }
   ],
   "source": [
    "# however, it is unable to infer vectors for unfamilar words\n",
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print('The word cameroon does not appear in this mode.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car : minivan --> 0.6907036304473877\n",
      "car : bicycle --> 0.5364484786987305\n",
      "car : airplane --> 0.42435577511787415\n",
      "car : cereal --> 0.13924746215343475\n",
      "car : communism --> 0.05820293724536896\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec supports several word similarities out of the box\n",
    "pairs = [\n",
    "    ('car', 'minivan'),\n",
    "    ('car', 'bicycle'),\n",
    "    ('car', 'airplane'),\n",
    "    ('car', 'cereal'),\n",
    "    ('car', 'communism')\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print(f'{w1} : {w2} --> {wv.similarity(w1, w2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a second to assess what just happened. We took a list of tuples containing the word `car` and a miscellaneous word. Some of these miscellaneous words were more similar to `car`, like `minivan`. Other words had next to nothing to do with `car`, i.e. `communism`. \n",
    "\n",
    "When we look at there similarity scores, we can see a trend. The less similar the word is to `car`, the lower the score is. So, remember that vector space we talked about before? That number essentially represents how close that respective word is to `car`. The higher the value, the closer it is; the lower the value, the further away the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even see which are the 5 most similar words to car or minivan\n",
    "# print(wv.most_similar(positive=['car', 'minivan'], topn=5))\n",
    "# the above line takes too much time on my local machine; if your\n",
    "# machine has a significant amount of memory, you can uncomment the line above\n",
    "# and run this command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training Your Own Model_\n",
    "\n",
    "In addition to pre-trained models like the Google News one above, we can create our own model as well. First, we'll need some data. Gensim includes some data sets, including the [Lee Corpus](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor), which we'll use to train our first model. \n",
    "\n",
    "This corpus is smaller than Google News, however, we'll implement a memory-friendly iterator that reads it in line-by-line, which is a better demonstration of how to handle a larger corpus.\n",
    "\n",
    "The `MyCorpus` class also gives us the capability, if we so choose, to do custom preprocessing. For example, we could decode a non-standard encoding, or lowercase the text, extract named entities, etc. All of this could be done inside the `MyCorpus` iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    '''An iterator that yields sentences (lists of str)'''\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            #assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an out-of-the-box model on the Lee corpus\n",
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our model, and we can use this in the same way to the Google model above. The main part of the model is `model.wv`, where `wv` stands for **\"word vectors\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector for word king\n",
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds\n",
      "of\n",
      "people\n",
      "have\n",
      "been\n",
      "forced\n",
      "to\n",
      "their\n",
      "homes\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "# retrieve the vocabulary of our model\n",
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Storing and loading models_\n",
    "\n",
    "There is an unfortunate downside to training models: they can take significant amounts of time. Once your model has been trained and it works (like you expect it to), you can save it to disk. This means that we won't have to spend time training it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # the model can now safely be stored in the filepath\n",
    "    # you can copy it to other machines, share it with others, etc\n",
    "    #\n",
    "    # To laod a saved model\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses pickle internally. In addition, you can load models created by the original C tool, both using its text and binary formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training Parameters_\n",
    "\n",
    "`Word2Vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "### `min_count`\n",
    "\n",
    "This is for pruning the internal dictionary. Words that appear only a few times in say, a billion-word corpus, are uninteresting. Plus, there's not enough data to make anything useful of them, so it's best to ignore them. \n",
    "\n",
    "The default value of `min_count` is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with our corpus, create model where a word has to appear at least 10 times\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `size`\n",
    "\n",
    "This is the number of dimensions $N$ of the $N$-dimensional space that `gensim` Word2Vec maps the words onto. A bigger size value requires more training data, but can lead to better (i.e. more accurate) models. For a start, values within the tens to the hundreds are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of N equal to 200 (default = 100)\n",
    "model = gensim.models.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `workers`\n",
    "\n",
    "This is the last major parameter, and is for parallelization, which speeds up training. The default number of `workers` is equal to 3. \n",
    "\n",
    "If you want a full list of parameters, you can check out this [link](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of workers equal to 4 (default = 3)\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Memory_\n",
    "\n",
    "`Word2Vec` models are stored as matrices (i.e. `NumPy` arrays) with each array being **vocabulary times size of floats**. Three matrices are held in RAM, so if your input contains 100,000 unique words, and has `size=200`, the model will require approximately `100,000 * 200 * 4 * 3 = ~229MB`. \n",
    "\n",
    "Additionally, there's a little extra memory needed for storing the vocabulary tree (which requires a few megabytes) but unless your words are extremely long strings, the footprint of this will be minimal compared to the three matrices above. \n",
    "\n",
    "## _Evaluating_\n",
    "\n",
    "Since `Word2Vec` is an unsupervised task, there's no \"good\" way to evaluate the result, as evaluation is also dependent on your end application. \n",
    "\n",
    "Google has released their testing set of 20,000 syntactic & semantic test examples that follow the __\"A is to B as C is to D\"__ task; this is provided in the `datasets` folder. \n",
    "\n",
    "An example of a syntactic analogy is `bad to words : good to ?`. There are a total of 9 types of syntactic comparisons in the dataset like plural nouns and nouns of opposite meaning.\n",
    "\n",
    "An example of a semantic question could be capital cities (`Paris to France : Tokyo to ?`) or family members (`brother to sister : dad to ?`). There are five types of these semantic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can access the Google test examples with the following command \n",
    "# model.accuracy('/datasets/questions-words.txt')\n",
    "# throws a FileNotFoundError, need to address this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Online training/Resuming Training_\n",
    "\n",
    "A more advanced technique is to load a model and continue training it with more sentences and new vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 65)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training Loss Computation_\n",
    "\n",
    "There is a parameter -- `compute_loss` -- that can be used to toggle computation of loss while training the `Word2Vec` model, which is stored in the model attribute `running_training_loss`. This can be retrieved using the function `get_latest_training_loss` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383308.5\n"
     ]
    }
   ],
   "source": [
    "# instantiate and train the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=1,\n",
    "    compute_loss=True,\n",
    "    hs=0,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# get the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
